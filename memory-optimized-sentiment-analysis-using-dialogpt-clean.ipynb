{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "2NK0aR9VvSK7",
    "outputId": "a7af9aa7-bc12-48bf-de09-c5518d737045"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UF0TwajvS1j",
    "outputId": "19112259-ef5a-4606-ae44-4dfed48c3443"
   },
   "outputs": [],
   "source": [
    "# Run this cell first to fix the bitsandbytes issue\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers accelerate\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-C9HEGOZvoKD",
    "outputId": "948737f2-929d-4845-b325-02ff6a7fa6e6"
   },
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -U bitsandbytes peft safetensors\n",
    "!pip install transformers datasets torch accelerate gradio pandas scikit-learn\n",
    "!pip install -U transformers accelerate\n",
    "\n",
    "# Create offload directory\n",
    "import os\n",
    "os.makedirs(\"./offload\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ All packages installed and offload directory created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AO4ddpcgV4or",
    "outputId": "119084c9-0d74-4a0e-eb4a-9245ca3fc185"
   },
   "outputs": [],
   "source": [
    "# Run this first to clear memory\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "!pip install transformers datasets torch accelerate gradio pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4cdcb040769648a8ac43705fc5d1d8a0",
      "3c6a6fc9045a4c3294b753231eb85c06",
      "919781d5192a443ba889f0de58f45304",
      "9b83382924074d14b9283c21f51ee9ee",
      "c661fcae019740e79706b6d914a7433c",
      "1ab26b403bcc429ea02ad08354799ea9",
      "74f5a7cc252e49e9a3648ad52af3f2d5",
      "dbe711f0f76f48dd899d93b4b933e120",
      "d23206ee051045498847edf1781cf21e",
      "9e25d6db831e48e981ef94d250446d2e",
      "e973d936952740bf942caa920197b6e7",
      "470404d221e64adb9d8734ca54ded943",
      "713477e224004ec58c4df5a1872f8299",
      "032f74f175404563b96966bfbd180635",
      "f64a4d963d494a5f9eb49c4b9ae45d8c",
      "2b6fb063c1bd4b94a7ec2860a4dca1d8",
      "d9fbe6eb9ab14de792c25566300a3c3c",
      "708c959ad26a489d96e70962c9813113",
      "63648ac3284142e995664d784522885a",
      "09384c8a0bb2443a9956ef29f6469970",
      "f3963b2f44f04f3ab9ce5af9d0c1b81f",
      "fa66d4cda19e47b1bd5c3ba804caa0ac",
      "85d120909aaa40cb87528378dfbc3309",
      "77729ca9bedb4c40b62c2dce83fe9d39",
      "9e5d2b72614241049e5726b3a985b7b9",
      "9614c8ff622e4693918061a860de0378",
      "cddf3ec4ce0c40c3975c875cf9e5cf34",
      "e89ff7e0482048e5af6beb23616d2041",
      "4b1eb21fb40c4a43b73b6441c3f52594",
      "97eb83ec562c4768a4f771143f171a52",
      "f2980162190f448980d787034b005ca7",
      "6d3b9592c51b4a7099b285ef36a82ca1",
      "0c83925a41c9484b8009ee401ce1cd96",
      "41764897b8c34ab2960737da8fbddd01",
      "cef1ca4ec3784266bee0d43c65a49202",
      "883689fb47044b9c8467e03789692242",
      "96a4a45c5f1342d280dd318d8758e790",
      "ddfa795b8e054a2a88bd6ddcf16046f8",
      "b84ae83bda2747ada49d906f9700aab3",
      "0210b966aa66449a97d8e8c895423143",
      "e1ff84c717f04bc88dffacc67eedb612",
      "cee69844f8534050822253d6c20957d2",
      "c1fd5f144fce49db95a873d984474a87",
      "21159b08ae154f4a8050db836005b142",
      "0ba490357a5042ba87812f810e35277f",
      "433edc9af7f14a288b57194b41680f94",
      "eeb12f476d314267bfea1205477384c0",
      "4472c2d2c8304bb5b46c51e77ec0fcae",
      "0d4f6852903c427fb72a80e4ecc74922",
      "6a3449b8677a46dcb1e3659ab327e190",
      "6b82bcd9e8cc4bbdb5386687d33ffe87",
      "19449357808042ed9198584ce42338d7",
      "6fe832a5790f4699816c0d2b82ba1fe8",
      "6c460826a97948448aa7fb71be680b45",
      "513baa723394430e9e724f7243b00fc0",
      "1297a479562b4dfc937c5141b502cc9c",
      "86517285558749c0855dfe0b3f9a4c58",
      "72659e38959e49eeb8a1fff7428e1aad",
      "af55b59d1a0e42c3b070ceb7d0584a9f",
      "d5597a8eb7c1498baeb9ef81c6df2aa9",
      "88f048b402754536b88780c20681ca01",
      "bec1997765f64fe7b90fad1365869cb0",
      "f0fee915b8ca4b718938465da30c7f00",
      "4e4b6094613249b1b1786d29317e1b17",
      "4136dac35450413ab853125cc255a576",
      "f2e02edbfaff4888a622526eb0f0e5c7"
     ]
    },
    "id": "WaAzkKXvyuUn",
    "outputId": "88ec8f27-bac8-498b-88d2-4ad62233c0f1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Memory management\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class ImprovedSentimentLLM:\n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-small\"):  # Better conversational model\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trained_model = None\n",
    "\n",
    "        # Enhanced sentiment patterns for better fallbacks\n",
    "        self.sentiment_patterns = {\n",
    "            'very_positive': {\n",
    "                'keywords': ['amazing', 'fantastic', 'wonderful', 'excellent', 'perfect', 'outstanding', 'brilliant', 'awesome', 'incredible', 'spectacular'],\n",
    "                'response': \"Very Positive üåü You're radiating amazing energy! Keep embracing these wonderful moments and let them fuel your day!\"\n",
    "            },\n",
    "            'positive': {\n",
    "                'keywords': ['happy', 'good', 'great', 'excited', 'joy', 'pleased', 'glad', 'cheerful', 'delighted', 'optimistic', 'fun', 'yay'],\n",
    "                'response': \"Positive üòä Your positive energy is contagious! It's beautiful to see you enjoying life's moments.\"\n",
    "            },\n",
    "            'very_negative': {\n",
    "                'keywords': ['hate', 'terrible', 'awful', 'horrible', 'disgusting', 'furious', 'devastated', 'miserable', 'depressed'],\n",
    "                'response': \"Very Negative üòî I hear you're going through something really tough. Consider reaching out to someone you trust or a professional for support.\"\n",
    "            },\n",
    "            'negative': {\n",
    "                'keywords': ['sad', 'angry', 'upset', 'bad', 'frustrated', 'annoyed', 'disappointed', 'worried', 'stressed', 'down'],\n",
    "                'response': \"Negative üòü It sounds like you're having a challenging time. Take a deep breath and remember that difficult feelings are temporary.\"\n",
    "            },\n",
    "            'neutral': {\n",
    "                'keywords': ['okay', 'fine', 'normal', 'alright', 'so-so', 'meh', 'whatever'],\n",
    "                'response': \"Neutral üòê You seem to be in a balanced emotional space. Sometimes that's exactly where we need to be.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU/CPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def setup_model_and_tokenizer(self):\n",
    "        \"\"\"Initialize model with 4-bit quantization for better efficiency\"\"\"\n",
    "        print(f\"üöÄ Loading improved model ({self.model_name})...\")\n",
    "\n",
    "        # Clear memory first\n",
    "        self.clear_memory()\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # 4-bit quantization config for memory efficiency\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        ) if torch.cuda.is_available() else None\n",
    "\n",
    "        # Load model with quantization\n",
    "        try:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "\n",
    "            print(\"‚úÖ Model loaded with quantization!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Quantization failed: {e}\")\n",
    "            # Fallback to regular loading\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            if torch.cuda.is_available():\n",
    "                self.model = self.model.cuda()\n",
    "            print(\"‚úÖ Model loaded (fallback mode)!\")\n",
    "\n",
    "        print(f\"Model parameters: ~{sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\")\n",
    "\n",
    "    def analyze_sentiment_patterns(self, text):\n",
    "        \"\"\"Analyze text using pattern matching for better fallbacks\"\"\"\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Count sentiment indicators\n",
    "        sentiment_scores = {\n",
    "            'very_positive': 0,\n",
    "            'positive': 0,\n",
    "            'very_negative': 0,\n",
    "            'negative': 0,\n",
    "            'neutral': 0\n",
    "        }\n",
    "\n",
    "        for sentiment_type, data in self.sentiment_patterns.items():\n",
    "            for keyword in data['keywords']:\n",
    "                if keyword in text_lower:\n",
    "                    sentiment_scores[sentiment_type] += 1\n",
    "\n",
    "        # Find dominant sentiment\n",
    "        max_score = max(sentiment_scores.values())\n",
    "        if max_score > 0:\n",
    "            dominant_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
    "            return self.sentiment_patterns[dominant_sentiment]['response']\n",
    "\n",
    "        return \"Neutral üòê I'm processing your message. Take a moment to reflect on how you're feeling right now.\"\n",
    "\n",
    "    def prepare_better_dataset(self, data_list):\n",
    "        \"\"\"Create better formatted dataset\"\"\"\n",
    "        training_texts = []\n",
    "        for item in data_list:\n",
    "            # Use a more structured format\n",
    "            prompt = f\"Human: {item['input_text']}\\nAssistant: {item['target_text']}<|endoftext|>\"\n",
    "            training_texts.append(prompt)\n",
    "        return training_texts\n",
    "\n",
    "    def create_optimized_dataset(self, training_texts):\n",
    "        \"\"\"Create optimized dataset with better examples\"\"\"\n",
    "        print(f\"Creating optimized dataset from {len(training_texts)} examples...\")\n",
    "\n",
    "        # Use more examples but keep them short\n",
    "        training_texts = training_texts[:16]  # More examples for better learning\n",
    "\n",
    "        # Tokenize with appropriate sequence length\n",
    "        tokenized_data = []\n",
    "        for text in training_texts:\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=200,  # Slightly longer for better context\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            tokenized_data.append({\n",
    "                'input_ids': encoded['input_ids'].squeeze().tolist(),\n",
    "                'attention_mask': encoded['attention_mask'].squeeze().tolist(),\n",
    "                'labels': encoded['input_ids'].squeeze().tolist()\n",
    "            })\n",
    "\n",
    "        dataset = Dataset.from_list(tokenized_data)\n",
    "        print(f\"‚úÖ Optimized dataset created with {len(dataset)} examples!\")\n",
    "        return dataset\n",
    "\n",
    "    def improved_fine_tune(self, training_texts):\n",
    "        \"\"\"Improved fine-tuning with better parameters\"\"\"\n",
    "        print(\"üî• Starting improved fine-tuning...\")\n",
    "\n",
    "        train_dataset = self.create_optimized_dataset(training_texts)\n",
    "\n",
    "        # Better training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./improved_model\",\n",
    "            num_train_epochs=2,  # More epochs\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=3e-4,  # Better learning rate\n",
    "            warmup_steps=5,\n",
    "            logging_steps=2,\n",
    "            save_steps=100,\n",
    "            save_strategy=\"no\",\n",
    "            logging_strategy=\"steps\",\n",
    "            report_to=\"none\",\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_num_workers=0,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            max_steps=20,  # More training steps\n",
    "            dataloader_pin_memory=False,\n",
    "            optim=\"adamw_torch\",\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "\n",
    "            print(\"üöÄ Training...\")\n",
    "            trainer.train()\n",
    "            print(\"‚úÖ Improved fine-tuning completed!\")\n",
    "\n",
    "            self.trained_model = self.model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            print(\"Using base model with pattern matching...\")\n",
    "            self.trained_model = self.model\n",
    "\n",
    "        self.clear_memory()\n",
    "\n",
    "    def clean_response(self, response):\n",
    "        \"\"\"Clean and improve model responses\"\"\"\n",
    "        # Remove repetitive patterns\n",
    "        response = re.sub(r'(Sentiment:\\s*)+', '', response)\n",
    "        response = re.sub(r'(Advice:\\s*)+', '', response)\n",
    "\n",
    "        # Remove incomplete sentences\n",
    "        sentences = response.split('.')\n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10 and sentence.count(' ') > 1:  # Filter out fragments\n",
    "                cleaned_sentences.append(sentence)\n",
    "\n",
    "        if cleaned_sentences:\n",
    "            response = '. '.join(cleaned_sentences[:2])  # Max 2 sentences\n",
    "            if not response.endswith('.'):\n",
    "                response += '.'\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    def generate_response(self, input_text):\n",
    "        \"\"\"Generate improved response with multiple fallback layers\"\"\"\n",
    "        if self.trained_model is None:\n",
    "            return \"Model not ready!\"\n",
    "\n",
    "        try:\n",
    "            prompt = f\"Human: {input_text}\\nAssistant:\"\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=150\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available() and next(self.trained_model.parameters()).is_cuda:\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.trained_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=60,\n",
    "                    temperature=0.7,  # Lower temperature for more focused responses\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = full_response.replace(prompt, \"\").strip()\n",
    "\n",
    "            # Clean the response\n",
    "            response = self.clean_response(response)\n",
    "\n",
    "            # If response is still poor, use pattern matching\n",
    "            if (not response or\n",
    "                len(response) < 10 or\n",
    "                'Sentiment:' in response or\n",
    "                response.count('Advice:') > 1):\n",
    "                response = self.analyze_sentiment_patterns(input_text)\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return self.analyze_sentiment_patterns(input_text)\n",
    "\n",
    "# Enhanced training data with better examples\n",
    "enhanced_sample_data = [\n",
    "    {\"input_text\": \"I feel sad and depressed today\", \"target_text\": \"Negative üò¢ I understand you're going through a difficult time. These feelings are valid, and it's important to reach out for support when you need it.\"},\n",
    "    {\"input_text\": \"I am really happy today\", \"target_text\": \"Positive üòä That's wonderful to hear! Your happiness is contagious. Keep embracing these joyful moments.\"},\n",
    "    {\"input_text\": \"I'm confused about everything lately\", \"target_text\": \"Neutral üòê Feeling confused is completely normal. Take things one step at a time and give yourself space to process your thoughts.\"},\n",
    "    {\"input_text\": \"This situation is absolutely terrible\", \"target_text\": \"Negative üò† I can hear your frustration. When things feel overwhelming, try breaking down the situation into smaller, manageable parts.\"},\n",
    "    {\"input_text\": \"Having an amazing day with friends\", \"target_text\": \"Positive üéâ That sounds fantastic! Good friends and positive experiences are precious. Treasure these beautiful moments.\"},\n",
    "    {\"input_text\": \"I don't know what to think anymore\", \"target_text\": \"Neutral ü§î Uncertainty can be uncomfortable, but it's also an opportunity for growth. Be patient with yourself as you figure things out.\"},\n",
    "    {\"input_text\": \"I hate how things are going\", \"target_text\": \"Negative üò§ Strong feelings like this show that something important to you needs attention. Consider what changes might help improve your situation.\"},\n",
    "    {\"input_text\": \"Everything is going perfectly right now\", \"target_text\": \"Positive ‚ú® How wonderful! These perfect moments are gifts. Soak up this positive energy and let it strengthen you.\"},\n",
    "    {\"input_text\": \"yay this was fun\", \"target_text\": \"Positive üéä I love your enthusiasm! It's great to hear you had such a fun experience. Keep seeking out those joyful moments!\"},\n",
    "    {\"input_text\": \"feeling okay I guess\", \"target_text\": \"Neutral üòê Sometimes 'okay' is exactly where we need to be. There's no pressure to feel anything more than what's authentic for you right now.\"},\n",
    "]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with improved setup\"\"\"\n",
    "    print(\"üß† Improved Memory-Optimized Sentiment Analysis\")\n",
    "    print(\"Using DialoGPT-small (~117M parameters) with 4-bit quantization!\")\n",
    "\n",
    "    # Initialize\n",
    "    llm = ImprovedSentimentLLM()\n",
    "    llm.setup_model_and_tokenizer()\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        print(\"Loading sentiment_dataset.csv...\")\n",
    "        df = pd.read_csv('sentiment_dataset.csv')\n",
    "        print(f\"‚úÖ Found {len(df)} entries\")\n",
    "\n",
    "        custom_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            custom_data.append({\n",
    "                \"input_text\": str(row['input_text'])[:150],\n",
    "                \"target_text\": str(row['target_text'])[:200]\n",
    "            })\n",
    "\n",
    "        if len(custom_data) < 5:  # If not enough custom data\n",
    "            custom_data.extend(enhanced_sample_data)\n",
    "\n",
    "        training_data = llm.prepare_better_dataset(custom_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Using enhanced sample data: {e}\")\n",
    "        training_data = llm.prepare_better_dataset(enhanced_sample_data)\n",
    "\n",
    "    # Improved training\n",
    "    llm.improved_fine_tune(training_data)\n",
    "\n",
    "    # Create improved UI\n",
    "    def predict(text):\n",
    "        if not text.strip():\n",
    "            return \"Please enter some text to analyze!\"\n",
    "\n",
    "        response = llm.generate_response(text[:300])\n",
    "        return response\n",
    "\n",
    "    # Enhanced Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=predict,\n",
    "        inputs=gr.Textbox(\n",
    "            lines=3,\n",
    "            placeholder=\"Share what's on your mind...\",\n",
    "            label=\"Your Message\"\n",
    "        ),\n",
    "        outputs=gr.Textbox(label=\"Sentiment Analysis & Response\", lines=4),\n",
    "        title=\"üí´ Enhanced Sentiment Analysis AI\",\n",
    "        description=\"Improved AI with better understanding and more natural responses!\",\n",
    "        examples=[\n",
    "            [\"yay this was so much fun today!\"],\n",
    "            [\"I'm feeling down and confused\"],\n",
    "            [\"Had an amazing time with my family\"],\n",
    "            [\"This is really frustrating me\"],\n",
    "            [\"I'm not sure how I feel right now\"],\n",
    "            [\"Everything is going perfectly!\"],\n",
    "            [\"I hate how complicated this is\"]\n",
    "        ],\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        .gradio-container {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ Launching enhanced interface...\")\n",
    "    interface.launch(\n",
    "        share=True,\n",
    "        debug=False,\n",
    "        quiet=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Memory check\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "        print(\"Using 4-bit quantization for memory efficiency!\")\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odDaTnNL0wif"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
