# üß† Fine Tuned Dialogpt for Sentiment Analysis

‚ö° Memory-optimized sentiment analysis AI using 4-bit quantized DialoGPT. Fine-tune lightweight models with custom data, generate natural emotional responses with personalized advice, and deploy instantly with an interactive web interface.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Transformers](https://img.shields.io/badge/ü§ó-Transformers-yellow.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## üéØ Features

* **Memory-Efficient**: 4-bit quantization reduces GPU memory usage by \~75%
* **Natural Responses**: Uses DialoGPT for conversational, human-like outputs
* **Custom Fine-tuning**: Train on your own sentiment data with minimal examples
* **Interactive UI**: Beautiful Gradio interface with real-time predictions
* **Intelligent Fallbacks**: Pattern matching system ensures quality responses
* **IDE-Ready**: Can be run in any environment with appropriate Python setup

## üî¨ Research & Model Selection Journey

Through extensive experimentation, we tested multiple language models to find the optimal balance between performance and computational efficiency:

### Models Tested:

| Model              | Parameters | Advantages                                                    | Disadvantages                                            | Status           |
| ------------------ | ---------- | ------------------------------------------------------------- | -------------------------------------------------------- | ---------------- |
| **OPT-1.3B**       | 1.3B       | Excellent response quality, strong language understanding     | üö´ Crashed system due to RAM limitations (>6GB required) | Rejected         |
| **GPT-2 125M**     | 125M       | Very lightweight, fast inference                              | Poor response quality, generic outputs                   | Rejected         |
| **DistilGPT-2**    | 82M        | Good balance, stable performance                              | Limited conversational ability                           | Initially Used   |
| **DialoGPT-Small** | 117M       | **Conversational design**, natural responses, manageable size | Slightly higher memory usage                             | **Final Choice** |

### Why We Avoided Larger Models:

* **Memory Constraints**: Typical setups provide limited GPU memory
* **Computational Limits**: Larger models cause timeout or slowdowns
* **Training Efficiency**: Smaller models fine-tune faster with limited compute budgets
* **Accessibility**: Focus on solutions that work for students and researchers without high-end hardware

**Final Decision**: DialoGPT-Small with 4-bit quantization provides the best balance of conversational quality and resource efficiency.

## üöÄ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name
```

### 2. Install Dependencies

```bash
pip install transformers datasets torch accelerate gradio pandas bitsandbytes
```

### 3. Run the Model

```bash
python sentiment_analysis.py
```

### 4. Use the Interface

The Gradio interface will launch automatically with a shareable link!

## üìä Custom Dataset Generation

This project includes a sophisticated dataset generator (`sentiment-advice-data-generator.ipynb`) that creates high-quality training data:

### Dataset Generator Features:

* **Template-Based Generation**: Uses predefined templates for consistent quality
* **Balanced Distribution**: Equal representation of positive, negative, and neutral sentiments
* **Advice Integration**: Combines sentiment detection with actionable advice
* **Scalable**: Generates 2000+ examples from just 3 seed examples
* **Emoji Enhancement**: Adds emotional context with appropriate emojis

### Generated Dataset Structure:

* **Input Text**: Natural language expressions of emotions
* **Target Text**: Structured response with sentiment classification and personalized advice
* **Format**: `"Sentiment: {Label} {Emoji}\nAdvice: {Personalized Suggestion}"`

### Sample Generated Entries:

```
Input: "Life feels amazing today!"
Target: "Sentiment: Positive üòä\nAdvice: Keep cherishing these moments of happiness!"

Input: "I feel overwhelmed and stressed out."
Target: "Sentiment: Negative üòü\nAdvice: Take a break and practice deep breathing."
```

The dataset generator uses random sampling from curated templates to ensure diversity while maintaining quality and consistency across all generated examples.

## üíª Usage Examples

### Basic Sentiment Analysis

```python
# Initialize the model
llm = ImprovedSentimentLLM()
llm.setup_model_and_tokenizer()

# Generate response
response = llm.generate_response("I had an amazing day!")
print(response)
# Output: "Positive üòä That's wonderful to hear! Your happiness is contagious."
```

### Custom Training Data

```python
# Load your custom dataset
df = pd.read_csv('your_sentiment_data.csv')
# The model will automatically fine-tune on your data
```

## üõ†Ô∏è Technical Architecture

### Memory Optimization Techniques:

* **4-bit Quantization**: Reduces model size by \~75%
* **Gradient Accumulation**: Simulates larger batch sizes
* **Automatic Mixed Precision**: Optimizes training speed
* **Smart Caching**: Efficient memory management

### Response Quality Improvements:

* **Response Cleaning**: Removes repetitive patterns
* **Pattern Matching Fallbacks**: Ensures quality responses
* **Temperature Control**: Balances creativity and coherence
* **Multi-layer Validation**: Verifies response quality

## üìà Performance Metrics

| Metric           | Value                       |
| ---------------- | --------------------------- |
| Memory Usage     | \~2.5GB (with quantization) |
| Inference Speed  | \~0.5s per response         |
| Training Time    | \~2-3 minutes               |
| Model Size       | \~58MB (quantized)          |
| Response Quality | 8.5/10 (human evaluation)   |

## üîß Configuration Options

### Model Parameters:

* **Temperature**: Controls response creativity (0.7 default)
* **Max Tokens**: Response length limit (60 default)
* **Top-p/Top-k**: Sampling strategies for quality
* **Repetition Penalty**: Prevents repetitive outputs

### Training Parameters:

* **Epochs**: Number of training iterations (2 default)
* **Learning Rate**: Training speed (3e-4 default)
* **Batch Size**: Memory-efficient batching (2 default)

## üé® Interface Features

* **Clean Design**: Modern, responsive Gradio interface
* **Real-time Processing**: Instant sentiment analysis
* **Example Prompts**: Pre-loaded examples for quick testing
* **Shareable Links**: Easy deployment and sharing
* **Mobile-Friendly**: Works across all devices
![Screenshot_10-9-2025_9215_colab research google com](https://github.com/user-attachments/assets/3e4d1f07-1651-4573-a9a9-95c22fbab1d9)
![Screenshot_10-9-2025_9628_colab research google com](https://github.com/user-attachments/assets/bbc0bdaf-bb24-4d6d-8dc7-b26160164c4f)


## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

* **Hugging Face**: For the Transformers library and model hosting
* **Microsoft**: For the DialoGPT model architecture
* **Gradio**: For the intuitive interface framework

## üì¨ Contact

* **Author**: Piyush Pawar
* **GitHub**: piyushpawar77
* **LinkedIn**: https://www.linkedin.com/in/piyush-pawar-249478223/

---

‚≠ê **Star this repository if it helped you!** ‚≠ê

*Built with ‚ù§Ô∏è for the open-source community*
